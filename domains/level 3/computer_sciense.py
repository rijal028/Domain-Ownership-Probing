DOMAIN_name = "computer_science"

TEXTS = [
    "Computational complexity classifies problems by asymptotic resource requirements.",
    "The class P contains decision problems solvable in polynomial time on deterministic machines.",
    "The class NP contains problems whose solutions can be verified in polynomial time.",
    "NP-completeness is established via polynomial-time reductions from known hard problems.",
    "The Cook–Levin theorem proves SAT is NP-complete and anchors NP-completeness theory.",
    "The polynomial hierarchy generalizes NP and co-NP into multiple quantifier levels.",
    "Randomized complexity classes such as BPP capture efficient probabilistic computation.",
    "Cryptographic security often assumes hardness of problems like integer factoring or discrete log.",
    "Approximation algorithms provide bounded-quality solutions for NP-hard optimization problems.",
    "Parameterized complexity analyzes tractability using problem-specific structural parameters.",

    "Automata theory models computation using finite automata, pushdown automata, and Turing machines.",
    "Regular languages are recognized by finite automata and described by regular expressions.",
    "Context-free languages are generated by context-free grammars and recognized by pushdown automata.",
    "The pumping lemma provides proofs of non-regularity and non-context-freeness for languages.",
    "The Church–Turing thesis states that computable functions are those computable by Turing machines.",
    "Decidability determines whether an algorithm exists to always answer a decision problem.",
    "The halting problem is undecidable, demonstrating limits of algorithmic computation.",
    "Reductions transfer hardness between problems via computable mappings.",
    "Formal verification uses logic and automata to prove correctness properties of programs.",
    "Model checking systematically explores state spaces to verify temporal logic specifications.",

    "Operating systems provide abstractions such as processes, threads, memory, and files.",
    "Virtual memory maps logical addresses to physical memory using paging mechanisms.",
    "Page replacement algorithms manage memory pressure using heuristics like LRU approximations.",
    "Concurrency control prevents race conditions using synchronization primitives and scheduling.",
    "Deadlock arises when circular wait and mutual exclusion conditions hold simultaneously.",
    "Kernel design balances performance, security, and isolation via privileged execution.",
    "Microkernels minimize trusted computing base by moving services to user space.",
    "File systems manage persistence using metadata structures and allocation strategies.",
    "Journaling ensures crash consistency by logging operations before applying them.",
    "Capability-based security restricts access by unforgeable references to resources.",

    "Distributed systems coordinate computation across unreliable networks and multiple nodes.",
    "Consensus protocols such as Paxos and Raft ensure agreement despite failures.",
    "The CAP theorem describes trade-offs between consistency, availability, and partition tolerance.",
    "Eventual consistency allows replicas to converge over time in distributed storage.",
    "Vector clocks capture causality relationships in distributed event ordering.",
    "Two-phase commit provides atomicity for distributed transactions but can block on failures.",
    "Byzantine fault tolerance handles adversarial nodes using quorum-based agreement mechanisms.",
    "Load balancing distributes requests across servers to optimize throughput and latency.",
    "Caching improves performance by reusing computed results and reducing backend load.",
    "Observability uses metrics, logs, and traces to diagnose behavior in complex systems.",

    "Machine learning algorithms learn patterns from data using optimization and generalization.",
    "Gradient descent minimizes loss functions through iterative parameter updates.",
    "Regularization reduces overfitting by penalizing model complexity in objective functions.",
    "Cross-validation estimates generalization performance using repeated data splits.",
    "Representation learning discovers useful features using neural networks and embeddings.",
    "Transformers model sequences using self-attention and feed-forward layers.",
    "Self-attention computes contextual token interactions through dot-product similarity.",
    "Pretraining learns general representations which can be adapted via fine-tuning.",
    "Information retrieval ranks documents using similarity measures in embedding spaces.",
    "Large-scale systems require efficient indexing and approximate nearest neighbor search."
]